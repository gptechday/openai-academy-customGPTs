
# CustomÂ GPTs â€” Resource Pack for workshop
*(curated links, code snippets & quickâ€‘reference tables)*  

---

## 1Â Â Official documentation ğŸ”—  

| Topic | Quick link | Why it matters |
|-------|-----------|----------------|
| **GPT Actions** | [OpenAIâ€¯â†’â€¯ActionsÂ Intro](https://platform.openai.com/docs/actions/introduction) | Register tools / backâ€‘end APIs for your CustomÂ GPT |
| **Dataâ€‘retrieval Actions** | [OpenAIâ€¯â†’â€¯DataÂ Retrieval](https://platform.openai.com/docs/actions/data-retrieval) | Enable RAG pipelines with external document stores |
| **OpenAPI Manifest Guide** | [OpenAIâ€¯â†’â€¯CreateÂ Action](https://platform.openai.com/docs/actions/creating-action) | JSON/YAML schema that describes each action endpoint |

---

## 2Â Â Security & guardrails ğŸŒ  

| Resource | Focus area | Usage |
|----------|------------|-------------|
| [**OWASPÂ TopÂ 10 for LLM AppsÂ (2025)**](https://owasp.org/www-project-top-10-for-large-language-model-applications/) | Critical LLM risks | Topâ€‘10 table & mitigation map |
| [**Guardrailsâ€‘AI**](https://github.com/guardrails-ai/guardrails) | Validate & censor LLM I/O | Example PII validator |
| [**Rebuff**](https://github.com/protectai/rebuff) | Promptâ€‘injection detection | â€œPlugâ€‘andâ€‘playâ€ selfâ€‘hardening wrapper |
| [**OWASP LLM cheatâ€‘sheet (PDF)**](https://genai.owasp.org/resource/ai-security-solution-cheat-sheet-q1-2025/) | Oneâ€‘page mitigations | Security best practices |
| [**DeepSeek jailbreak caseâ€‘study â€“ WIRED**](https://www.wired.com/story/deepseeks-ai-jailbreak-prompt-injection-attacks/) | Realâ€‘world failure | Redâ€‘teaming anecdote |

---

## 3Â Â Openâ€‘source libraries & SDKs ğŸ› ï¸  

| Library | Install | Capability |
|---------|---------|------------|
| **LangChain** | `pip install langchain` | Orchestrate RAG flows & tools |
| **LlamaIndex** | `pip install llama-index` | Document loaders + vector stores |
| **Guardrailsâ€‘AI** | `pip install guardrails-ai` | Declarative guards |
| **OpenAIÂ Python** | `pip install openai` | Core SDK + Actions |
| **Rebuff** | `pip install rebuff` | Promptâ€‘injection wrapper |

---

## 4Â Â Code snippet â€” Adding Guardrails  

```python
from guardrails import Guard
from guardrails.hub import DetectPII
import openai

prompt_guard = Guard().use(DetectPII(on_fail="exception"))

def safe_completion(user_input: str):
    prompt_guard.validate(user_input)  # ğŸ”´Â Validation before GPT
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": user_input}],
    )
    return response.choices[0].message.content  # ğŸŸ¢Â Return safe text
```

---

## 5Â Â Sample Action Manifest (excerpt)  

```yaml
openapi: 3.1.0
info:
  title: CalendarÂ Assistant API
  version: "1.0.0"

paths:
  /create_event:
    post:
      operationId: createEvent
      summary: Create a GoogleÂ Calendar event via proxy
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/Event"
      responses:
        "200":
          description: Event created

components:
  schemas:
    Event:
      type: object
      required: [title, start_time, end_time, attendees]
      properties:
        title:       { type: string }
        start_time:  { type: string, format: date-time }
        end_time:    { type: string, format: date-time }
        attendees:
          type: array
          items: { type: string, format: email }
```

---

## 6Â Â Evaluation & redâ€‘teaming checklist âœ…  

1. **Static scans** â€“ OWASP risk scanner on the manifest.  
2. **Dynamic tests** â€“ fuzz with Rebuff & hostileâ€‘prompt corpus.  
3. **Simulated tool abuse** â€“ attempt your own function hijack.  
4. **Audit logs** â€“ ensure every action call has traceâ€‘ID & userâ€‘ID.  
5. **Regressions** â€“ rerun tests after each model update.  

---

## 7Â Â Further reading & courses ğŸ“  

* [OpenAIÂ Cookbook â€” GPTs & Actions](https://github.com/openai/openai-cookbook)  
* Stanford CS25Â LectureÂ 4 â€” *LLM Systems &Â Safety* (YouTube)  
* Coursera â€œBuilding SecureÂ AI Appsâ€ specialization  
* [NISTÂ IRÂ 8472 â€” Adversarial Testing of AI Systems](https://doi.org/10.6028/NIST.IR.8472)  

---

---

Â©Â 2025Â MuntaserÂ Syed â€” MITâ€‘licensed for workshop attendees.
